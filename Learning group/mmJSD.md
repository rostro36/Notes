# [Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence](https://arxiv.org/pdf/2006.08242.pdf)

## Introduction
Data can come from multiple modalities (sources) and learning those modalities is obviously harder than only having one source, as in general, we do not know from which source the current sample comes from. The easiest fix for this problem is by labelling each sample, but this labelling is too resource intensive, therefore a self-supervised process is preferred.
## Model wish-list
A good model should:
1. **scale** well to handle many modalities efficiently and not just transferring *M+1* modalities via some expensive process to *M* modalities. Until you reach one modality.
1. handle **missing data** well, so that the model can be robust in a non-perfect world.
1. **gain information** compared to a non-learning model as well as to a simple unimodal model.
1. use **no assumptions**, so that it can be used in as many situations as possible and can be specified further if information is available.

## General scenario & ELBO
The dataset *X* consists of *N* i.i.d.  samples, where each sample consists of *M* modalities, which are generated by some random process, that involves a hidden random variable *z*, where inter-modality dependencies are unknown.

Now, the interesting quantity, that we use to fit the model is the (log-)likelihood of the data:

log(p_&theta;({*X*^*i*} *i* < *N*))=&Sigma;_(*i* < *N*) log(p _&theta;(*X*^*i*)),

where log(p_&theta;(*X*^*i*))=[KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (q_&phi;(*z*|*X*^*i*)||p_&theta;(*z*|*X*^*i*))+[ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound) (&theta;,&phi;,*X*^*i*),

with [ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound) (&theta;,&phi;,*X*^*i*)=E_q_&phi;(*z*|*X*^*i*)\[log(p_&theta;(*X*^*i*|*z*))]-[KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (q_&phi;(*z*|*X*^*i*)||p_&theta;(*z*))

The benefit of using an ELBO is that it is computationally tractable and therefore also more efficiently optimised.

(Another) hard part now comes from the fact, that we may not know all the modalities and only *K*, so we can not answer the true posterior p_&theta;(*z*|*X*^*i*), but only the variational function q_&phi; _*K*(*z*|*X*^*i* _*K*). With this the ELBO has to be adjusted to:

[ELBO](https://en.wikipedia.org/wiki/Evidence_lower_bound) (&theta;,&phi; _*K*,*X*^*i*)=E_q _&phi; _*K*(*z*|*X*^*i* _*K*)\[log(p _&theta;(*X*^*i*|*z*))]-[KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (q _&phi; _*K*(*z*|*X*^*i* _*K*)||p _&theta;(*z*))

## Scalability through Product-of-Experts
Computing the probability for each combination of the *M* modes gives 2^*M* combinations and is therefore not feasible. A better way to do it is by computing each of the *M* modes and then try to arrange them in a joint approximation distribution.

Such as the **Product-of-Experts**(PoE), which adds distributions together, which means that in the case of Gaussian modes, the product is also a Gaussian.

Instead of the PoE, the **Mixture-of-Experts** (MoE) was also presented and is an arithmetic mean function, which gives better results for individual experts, but uses much more resources, as the KL-divergence has no closed solution.

## Multimodal Jensen-Shannon-Divergence model (mmJSD)
To get to the finished product, there are some steps involved, that relate it to the conventional ELBO and show some knobs to turn.
### Finished product
The model does not need additional training objectives, supervision(labelling) or importance sampling (which is computationally expensive) and stays scalable.

1. &pi;_i is the probability that the *i*-th of the *M+1* produced the sample (or rather the weight of the influence of the *i*-th component)
1. We define the [Jensen-Shannon-Divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) for *M+1* distributions:

**JS**^(*M+1*)_&pi;({q_j(*z*) ,*j*<*M*+1)=&Sigma; j<*M+1* &pi;_j [KL](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (q_j(*z*)|f_A({q_v(*z*)}, v<*M+1*)),

where f_A is a mixture distribution.

With this Jensen-Shannon-Divergence, we define a new ELBO:

ÊLBO(&theta;,&phi;,*X*)=E_q_&phi;(*z*|*X*)\[log(p_&theta;(*X*|*z*))]-**JS**^(*M+1*)_&pi;({q _&phi; _*x* _*j*(*z*|*x* _*j*),*j*<*M*) p _&theta;(*z*)
### Dynamic Prior
Instead of the simple unimodal approximation q_&phi;_*j*(*z*|*x* _*j*), we can use a **multimodal dynamic prior**, that models the shared factors between the experts better: p_f(*z*|*X*)=f({q_&phi; _v(*z*|*x*_v)} *v*<*M*, p_ &theta;(*z*)), where *f* is some function that works on the q_&phi;

Using the **MoE** as *f* gives the finished product of the section above.
### Modality-specific Latent Subspaces
An extension of the previous model extends the latent space *z* into a tuple of two parts, **S** which is distinct for each mode and every sample and **c**, which is shared among all modularities. *S* and *c* are considered independent given *X*.

The extended ÊLBO(&theta;,&phi;,*X*) now is: &Sigma; *j*<*M* E_q_&phi;_*c*(*c*|*X*)\[E_q _&phi; _*s* _*j*(*s* _*j*|*x*_ *j*)\[log(p_ &theta;(*x* _*j*|*s*_ *j*,*c*))]]-&Sigma; *j*<*M* D_KL(q _&phi; _*s* _*j*(*s* _*j*|*x* _*j*)||p _&theta;(s_j))-**JS**^(*M*+1)_&pi;({q _&phi; _*c* _ *j*(*c*|*x* _*j*)} *j*<*M*,p _&theta;(*c*))
## Experiments
mmJSD gets compared to [MVAE](http://arxiv.org/abs/1802.05335) and [MMVAE](https://arxiv.org/abs/1911.03393). For the discriminative capabilities, the latent representations are evaluated using linear classifiers on the unimodal and multimodal posterior approximations. To evaluate the generative performance, the generated samples are rated according to their quality and coherence across all modalities. Conditional generation should be coherent with the input data and randomly generated data should be coherent within each other. To test the coherence a classifier was used which was trained on the original unimodal training set. Precision defines the quality and the recall defines the diversity of the generated samples.

The two test stages are the MNIST handwritten digits dataset and the CelebA faces dataset, where each image has some adjacent textual description.

The MNIST dataset has three modalities and shows that the two MVAEs have trouble of incorporating a third modality, also it shows that modality specific latent spaces improve the quality of the generated samples. The CelebA experiments again favour mmJSD and show that rare and subtle attributes are learnt worse, compared to more common attributes.

## Further questions
- Finding a new *f* for the dynamic prior
- Applying the mmJSD in the medical domain, as with the more data they collect now, more modalities are formed