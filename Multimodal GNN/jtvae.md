# [Junction Tree Variational Autoencoder for Molecular Graph Generation](https://arxiv.org/abs/1802.04364)
## Introduction
The final goal of this niche is to find target molecules with desired chemical properties. This consists of encoding the properties and then decoding this latent representation into a valid molecule.

One can do the decoding by using a [SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) representation, which then can get used to deterministically decode this representation. But this representation was not designed for similarity, therefore autoencoders can not be used for learning smooth molecular embeddings. It is also hard check molecule validity on SMILES.

Instead of generating a molecule atom by atom, this paper generates molecules by atom structure, which are modelled as tree nodes. So a molecule is modelled as a graph of \(overlapping\) graph components.

The graph components are known a priori and consist of 780 single edges and rings.
## Architecture
The representation of a molecule has two parts, both generated by graph encoders:
- *z\_T* &rightarrow; encodes the tree structure
	- is a [junction tree](https://en.wikipedia.org/wiki/Junction_tree_algorithm), where clusters can be contracted to form a tree
- *z\_G* &rightarrow; encodes the fine-grained connectivity, given the tree structure.

The graph to pass messages around on consists of atoms as vertices and bonds as typed edges. The graph encoding network is a simple GNN with addition as vertex aggregation and mean as graph aggregation. The mean and variance are then predicted from the graph hidden state.

The graph can be decomposed into a junction tree, where there are two phase types, from the root to the leaf and from the leaf to the root. This is done using a GRU and the aggregation is again addition\/average at the root.

Using *z\_T*, the junction tree can be generated in a depth first order starting at the root. And updating the same GRU from the decomposition above with new decisions, either the hidden state of the child or \"no new child\".

To test whether a new node should be generated the node features, the inward messages and *z\_T* are used. The label then is predicted using softmax based on *z\_T* and the hidden state of the possible edge. 

Teacher forcing \(predicting, but then using the ground truth for the next step\) is used to ensure correct histories.

From the tree, the graph is generated from the root down. This is undeterministic as the different structures can combine in many different ways. This is done using sampling from a score, which is computed using a GNN in the tree so far.

To ensure validity each component has only a set of allowed next components.

## Experiments
The runtime is about linear in the number of clusters, which scales nicely to large graphs. All generated molecules are valid and also adhere better to constraints.